## 互联网数据挖掘作业一： PageRank 计算




### 一、简单介绍

​	按照维基百科的编辑规则，对数据进行预处理，建立形如 [标题:[出边列表]] 的字典并保存。然后读取，建立字典，按照PageRank 随机游走算法进行迭代计算每个标题的pr值，达到更新阈值或者迭代次数限制后结束迭代，进行排序后保存并分析数据排序结果。使用python语言完成任务。

### 二、数据处理

#### 1. 处理目的

​	将xml文本格式的文件处理成为形如 {标题1: [出边列表], 标题2: [出边列表]……} 的字典的形式。（出边列表指此标题页面中的对应link。）

#### 2. 处理规则

​	1）第一种方式认为：只要是此标题页面中存在的链接，而且此链接是一个标题（存在于 https://en.wikipedia.org/wiki/ 中），就认为这个链接有效，计入标题-出边字典中。这种方式下称为 **链接式**

注：链接式会将：

* 文本中形如 {{ISBN|022628705X}} 的编辑文字代表的链接指向："International Standard Book Number"页面
* 允许存在：[[File:A]]  这样的文件链接（但最后会因为不存在这样的key而删去）
* 允许存在：[[Wiktionary:A]] 这样的链接，但最后也会删去。
* 允许存在[['Wikipedia:Articles for deletion]] 这样维基内部的链接。（最后会有很多这样的链接）

​	2）第二种方式认为：只有正常的我们关注的词语页面才算本次作业我们研究的链接。即出去上面的这些特殊的辅助性的标识链接。这种方式下称为 **标准式**

​	**两种方式共同的处理规则：**

1. 大部分链接是使用正常的两个中括号括起来的，形如[[A]]，这时候只需要取中括号内部的标题。
2. 有些链接形如[[A|B]]，这意味着真实的标题是A，页面想要显示B，这时候只取A作为链接。
3. 有些链接（以watermelon为例）形如：[[watermel]]on，是一种容错式的不规范的写法。这时候需要用正则表达式也得到中括号后紧跟着的字母。
4. 有些链接是定位到某一页面的某一章节的，形如[[A#B]]，这时候认为此链接是指向A的。
5. 因为每个标题都应该是以大写字母开头的，而允许编辑文本的人写小写字母，实际点击的时候会自动转化。因此我们处理时需要全部修改为大写字母。
6. 一些页面进行了重定向，这些页面文本内容已经少有链接。因此为了方便，建立一个页面名字的重定向字典：{原名字: 现名字，现名字：现名字}，遇到重定向页面的文本时，弃掉文本内容，取其名字添加到字典，以备有的页面指向原来名字的页面。
7. 形如 [[Category:Character sets]] 标志此页面的分类，但是不显示在页面上，因此本着模拟点击的原则，忽略此链接。而 形如[[:Category:Character sets]]则会显示到页面上，因此收取此链接。
8. 上面几条规则的混合链接
9. 一些琐碎的其他处理，比如：[[Special:BookSources]]，[[Help:IPA]]等。

注：上述处理方法参考了：维基百科的编辑规则 https://en.wikipedia.org/wiki/Help:Wikitext 。

#### 3. 具体处理方法

1. 因为文件较大，所以可以采取按行读取或分段读取。这里采取了按行读取的方式。
2. 使用正则表达式结合上面的处理规则进行匹配标题和链接。正则表达式如下：

```python
titleP = re.compile('<title>(.+?)</title>') # 用来匹配正常标题
retitleP = re.compile('<redirect title="(.+?)" />') # 用来判断此页面是否重定向
outlinkP = re.compile('\[\[([^\[\]]*)\]\](\w*)') # 用来得到完整的链接（会补全中括号后面的单词剩余字母）
```

3. 进行其他上述处理规则的后续处理
4. 最后，对字典里的每一个出边列表进行检查，将每个标题修改为重定向后的名字，然后排除非正常页面的出边。

```python
for item in graph.items():
	tmp = list(map(lambda x: redict[x], list(filter(lambda x: x in redict.keys(), item[1]))))
    graph[item[0]] = list(filter(lambda x: x != item[0], tmp))
for item in graph.items():
    graph[item[0]] = list(filter(lambda x: x in graph.keys(), item[1]))
```

### 三、计算PageRank值

采用改进后使用随机游走的PageRank算法。以一定概率重新开始浏览一个新网页。

公式为：$\pi = \alpha P^T \pi + (1-\alpha)\frac{1}{N}I $      (1)

因为要储存百万x百万的稀疏矩阵有些浪费和困难，因此使用遍历迭代的算法。

* 对每个结点生成入边集合和出边个数统计
* 初始化每个结点的pr值为 $10^{-6}$
* 遍历每个结点，使用公式$PR(A) =\frac{1-\alpha}{N} + \alpha\Sigma_{v∈B_A}{\frac{PR(v)}{L(v)}} $ （2）进行更新 结点A，$\alpha$ 设置为0.85
* 重复迭代上一步，直到结点的pr值平均变化幅度小于 $10^{-7}$ 或者迭代次数大于32，停止迭代。（这两个数值根据实验折中取得）
* 对pr值进行排序  


#### 具体实现

计算PageRank部分（省略部分细节）：

```python
while(loss > epsilon and it < maxit):
        for item in inedge.items():
            tmp = (1-p)/mylen
            for inlink in item[1]:
                tmp = tmp + pr[inlink]/outnum[inlink]                
                newpr[item[0]] = tmp
            loss = loss + abs(pr[item[0]]-newpr[item[0]])
        pr = newpr
```

#### 复杂度分析

 ##### 空间复杂度：

由于稀疏矩阵的特性，利用Python的字典存储有向图，用邻接表进行存储空间复杂度为$O(V+E)$。

##### 时间复杂度：

迭代实现公式$(2)$，时间复杂度是 $O(nE)$。n为迭代次数。

### 四、结果分析

做了两组实验（针对两种预处理的方式），并侧重标准式进行了一些分析

#### 1. 链接式的实验结果：

##### 前20名：

```t
[('Wikipedia:Articles for deletion', 0.033627749930642326),
 ('International Standard Book Number', 0.02350619090262764),
 ('Wikipedia:Redirects for discussion', 0.018462720070254273),
 ('Wikipedia:Wikidata', 0.013603622234000462),
 ('United States', 0.012977950575865094),
 ('Wikipedia:Deletion review', 0.00906873461287991),
 ('Race and ethnicity in the United States Census', 0.008115475875935838),
 ('World War II', 0.006584397498760861),
 ('United Kingdom', 0.006551274312838762),
 ('Wikipedia:Categories for discussion', 0.006242440499744922),
 ('The New York Times', 0.006234456417230707),
 ('France', 0.005154010733036284),
 ('Help:Reverting', 0.004961648885141353),
 ('New York City', 0.004552479703211916),
 ('Latin', 0.004420644233638752),
 ('Wikipedia:WikiProject Missing encyclopedic articles', 0.004419028194532731),
 ('Canada', 0.00440282434058515),
 ('Germany', 0.00435046090372021),
 ('World War I', 0.004215156667846257),
 ('England', 0.003912131411012085)]
```

##### 分析

可以看到，前20名中，大多数是与Wikipedia相关的页面，因为大多数页面会存在指向这些分类、删除、数据等的Wikipedia的页面链接。除此之外，其他的页面与国家、二战、一战、城市、著名杂志相关（这些在标准式中更加明显，放到那里分析）。

这里来看一下与Wikipedia相关的页面的入边与出边链接（以Wikipedia:Articles for deletion为例）：它有45509个入边，31个出边，可见“当之无愧”。而排名第五的United States，有82423个入边，有936个出边，感觉还好。

除此之外还需要考虑他们入边的pr值，篇幅原因，放在标准式那里进行分析。

#### 2. 标准式实验结果

##### 1)  前21名pr值及其出入边数量

| 标题  |  入边  |  出边  |  pr值  |
| ---- | ---- | ---- | ---- |
| United States | 91574 | 965  | 0.059148355236519984 |
| World War II | 43135 | 537  | 0.033424702772142774 |
| United Kingdom | 36305 | 786  | 0.032016080968632954 |
| The New York Times | 41312 | 200  | 0.03176957417791156 |
| Race and ethnicity in the United States Census | 174321 | 41   | 0.029494280259431786 |
| Latin | 13042 | 242  | 0.027249125846883836 |
| Germany | 28188 | 775  | 0.02164357748353512 |
| New York City | 35857 | 816  | 0.02150896968453799 |
| Catholic Church | 18197 | 433  | 0.019903034619488093 |
| India | 18770 | 504 | 0.01896829255900096 |
| London | 27474 | 557 | 0.018341030505856577 |
| Italy | 19806 | 840 | 0.016948372304744646 |
| China | 14276 | 582 | 0.016452351509638696 |
| Soviet Union | 17297 | 373 | 0.01643261020183136 |
| England | 29776 | 931 | 0.015964825956728528 |
| Washington, D.C. | 16577 | 308 | 0.015494233578940432 |
| Oxford University Press | 7373 | 117 | 0.015488598165918683 |
| Japan | 20874 | 553 | 0.014999433081951105 |
| French language | 9697 | 235 | 0.014704498040385951 |
| Middle Ages | 8211 | 553 | 0.01352687508115884 |
| Russia | 11990 | 835 | 0.013496560036553399 |

这样看上去正常了很多，大都是国家、地区、著名杂志和出版社。比较好玩的有：Race and ethnicity in the United States Census、Oxford University Press、Middle Ages 这几个。

这次United States有91574个入边，965个出边，比前面链接式多了一些，是因为一百万个页面中去除了wiki相关的影响。

##### 2)  前1000个页面的pagerank与出入边值的变化

1000之后变化不大了，所以没有绘出.

可以看到，出边规律性较小，但还是总体有下降的趋势（与词的常用性有关），入边下降明显，与pr值趋势差不多。

<img src="${mdImageFile}/image-20201029231352421.png" alt="image-20201029231352421" style="zoom:80%;" />

##### 3)  随迭代次数，pr平均修正值的变化：（以迭代32次为例）

可以看到，pr到迭代5次之后，变化就已经比较小了。（初始值为 $10^{-6}$）

<img src="${mdImageFile}/image-20201029232153128.png" alt="image-20201029232153128" style="zoom:80%;" />

##### 4)  迭代10次需要的时间

​	136.4694676399231 seconds，可能会因为机器的差异而浮动。

### 五、关于代码

预处理代码在dataProcess.py, 计算PageRank在pagerank.py,  可以分别运行，也可以直接运行 runpr.sh

代码均有注释，可以结合报告。

### 六、总结与感想

大致任务：处理数据+写PageRank算法+分析数据+写报告，详细如上。

在处理数据上花了很多时间，先读wikitext链接，总结了很多琐碎的格式，最后发现还是舍去结果看起来比较正常。

找了好久redirect的bug（一开始以为是类似重命名，最后发现是类似作废的一个页面）。

最后报告还是写的很舒服的。









